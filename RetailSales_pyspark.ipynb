{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RetailSales_pyspark.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5ide5Xiq1D5t9YTsGFwTf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arifmhassan/Spark-Python-Data-Ingestion/blob/master/RetailSales_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlgHrhnh5s3s",
        "colab_type": "text"
      },
      "source": [
        "#Aggregations\n",
        "Primary Data source=online-retail-dataset.csv\n",
        "purpose: Aggregation using groupby, windows, rollup, sql like dense ranking\n",
        "\n",
        "Question\n",
        "*   dataframe read  the data online-retail-dataset.csv with 5 partition\n",
        "*   Data analysis count distinct e.g. stockcode, min quantity, max quantity, sum of quantity, sum of revenue by Country, CUstomer, date, hour  etc\n",
        "\n",
        "*   Whick date has the maximum  purchase quantity using dense_rank in sql and python\n",
        "*   group by accross country/date/ quantity using rollup and cube\n",
        "*    group by accross country/date/ quantity using pivot to get country listing in a column\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbxHIFBwJE7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize spark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz \n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz \n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGsX7jNnNUhB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "470ee030-8f1c-4ce3-99f9-27dc91fa1dce"
      },
      "source": [
        "# Source  git branch clone\n",
        "\n",
        "%%bash\n",
        "#-b my-branch\n",
        "git clone -b master https://github.com/databricks/Spark-The-Definitive-Guide.git master"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'master'...\n",
            "Checking out files:   5% (87/1738)   \rChecking out files:   5% (96/1738)   \rChecking out files:   6% (105/1738)   \rChecking out files:   6% (106/1738)   \rChecking out files:   6% (118/1738)   \rChecking out files:   7% (122/1738)   \rChecking out files:   7% (131/1738)   \rChecking out files:   8% (140/1738)   \rChecking out files:   8% (141/1738)   \rChecking out files:   8% (152/1738)   \rChecking out files:   9% (157/1738)   \rChecking out files:  10% (174/1738)   \rChecking out files:  11% (192/1738)   \rChecking out files:  12% (209/1738)   \rChecking out files:  13% (226/1738)   \rChecking out files:  14% (244/1738)   \rChecking out files:  15% (261/1738)   \rChecking out files:  16% (279/1738)   \rChecking out files:  17% (296/1738)   \rChecking out files:  18% (313/1738)   \rChecking out files:  19% (331/1738)   \rChecking out files:  20% (348/1738)   \rChecking out files:  21% (365/1738)   \rChecking out files:  22% (383/1738)   \rChecking out files:  23% (400/1738)   \rChecking out files:  24% (418/1738)   \rChecking out files:  25% (435/1738)   \rChecking out files:  26% (452/1738)   \rChecking out files:  27% (470/1738)   \rChecking out files:  28% (487/1738)   \rChecking out files:  29% (505/1738)   \rChecking out files:  30% (522/1738)   \rChecking out files:  31% (539/1738)   \rChecking out files:  32% (557/1738)   \rChecking out files:  33% (574/1738)   \rChecking out files:  34% (591/1738)   \rChecking out files:  35% (609/1738)   \rChecking out files:  36% (626/1738)   \rChecking out files:  37% (644/1738)   \rChecking out files:  38% (661/1738)   \rChecking out files:  39% (678/1738)   \rChecking out files:  40% (696/1738)   \rChecking out files:  41% (713/1738)   \rChecking out files:  42% (730/1738)   \rChecking out files:  43% (748/1738)   \rChecking out files:  44% (765/1738)   \rChecking out files:  45% (783/1738)   \rChecking out files:  46% (800/1738)   \rChecking out files:  47% (817/1738)   \rChecking out files:  48% (835/1738)   \rChecking out files:  49% (852/1738)   \rChecking out files:  50% (869/1738)   \rChecking out files:  51% (887/1738)   \rChecking out files:  52% (904/1738)   \rChecking out files:  53% (922/1738)   \rChecking out files:  54% (939/1738)   \rChecking out files:  55% (956/1738)   \rChecking out files:  56% (974/1738)   \rChecking out files:  57% (991/1738)   \rChecking out files:  58% (1009/1738)   \rChecking out files:  59% (1026/1738)   \rChecking out files:  60% (1043/1738)   \rChecking out files:  61% (1061/1738)   \rChecking out files:  62% (1078/1738)   \rChecking out files:  63% (1095/1738)   \rChecking out files:  64% (1113/1738)   \rChecking out files:  65% (1130/1738)   \rChecking out files:  66% (1148/1738)   \rChecking out files:  67% (1165/1738)   \rChecking out files:  67% (1178/1738)   \rChecking out files:  68% (1182/1738)   \rChecking out files:  69% (1200/1738)   \rChecking out files:  70% (1217/1738)   \rChecking out files:  71% (1234/1738)   \rChecking out files:  72% (1252/1738)   \rChecking out files:  73% (1269/1738)   \rChecking out files:  74% (1287/1738)   \rChecking out files:  75% (1304/1738)   \rChecking out files:  76% (1321/1738)   \rChecking out files:  77% (1339/1738)   \rChecking out files:  78% (1356/1738)   \rChecking out files:  79% (1374/1738)   \rChecking out files:  80% (1391/1738)   \rChecking out files:  81% (1408/1738)   \rChecking out files:  82% (1426/1738)   \rChecking out files:  83% (1443/1738)   \rChecking out files:  84% (1460/1738)   \rChecking out files:  85% (1478/1738)   \rChecking out files:  86% (1495/1738)   \rChecking out files:  87% (1513/1738)   \rChecking out files:  88% (1530/1738)   \rChecking out files:  89% (1547/1738)   \rChecking out files:  90% (1565/1738)   \rChecking out files:  91% (1582/1738)   \rChecking out files:  92% (1599/1738)   \rChecking out files:  93% (1617/1738)   \rChecking out files:  94% (1634/1738)   \rChecking out files:  95% (1652/1738)   \rChecking out files:  96% (1669/1738)   \rChecking out files:  97% (1686/1738)   \rChecking out files:  98% (1704/1738)   \rChecking out files:  99% (1721/1738)   \rChecking out files: 100% (1738/1738)   \rChecking out files: 100% (1738/1738), done.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jto0OXTWN64N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check data quality\n",
        "%cat master/data/retail-data/all/online-retail-dataset.csv | head -n 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvtsy446PeFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the file\n",
        "filename=\"master/data/retail-data/all/online-retail-dataset.csv\"\n",
        "dataframe=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(filename).coalesce(5)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_xrW5-AQSvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "b3b6914a-53d7-4f90-8ccf-2ed12a4f9a84"
      },
      "source": [
        "#display data \n",
        "#create view\n",
        "dataframe.show(5)\n",
        "dataframe.createOrReplaceTempView(\"retailsales\")\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
            "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
            "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
            "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
            "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
            "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niboL-BsSTY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "c4bf11e0-6209-4b35-a281-fe43a7dbffc5"
      },
      "source": [
        "# Data Analysis\n",
        "# query data sets\n",
        "spark.sql(\"select * from retailsales where Country like '%United%' order by Country asc limit 2\").show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+---------+--------------------+--------+---------------+---------+----------+--------------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|             Country|\n",
            "+---------+---------+--------------------+--------+---------------+---------+----------+--------------------+\n",
            "|   543911|    21485|RETROSPOT HEART H...|       6|2/14/2011 12:46|     4.95|     17829|United Arab Emirates|\n",
            "|   543911|    48138|  DOORMAT UNION FLAG|       2|2/14/2011 12:46|     7.95|     17829|United Arab Emirates|\n",
            "+---------+---------+--------------------+--------+---------------+---------+----------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjBrs8gFENLV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "32c6f8e7-3a60-4927-f152-a25976dbc708"
      },
      "source": [
        "dataframe.printSchema()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- InvoiceNo: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: string (nullable = true)\n",
            " |-- UnitPrice: double (nullable = true)\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msAxFeocEpxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Analysis Summary\n",
        "print(dataframe.select (\"Country\").count())\n",
        "print(spark.sql(\"select count( Country) Country  from retailsales\").show())\n",
        "\n",
        "# distinct \n",
        "print(dataframe.select (\"Country\").distinct ().count())\n",
        "print(spark.sql(\"select count (distinct Country) from retailsales\").show())\n",
        "\n",
        "# first and last , max, min \n",
        "from pyspark.sql.functions import first, last, min, max, countDistinct\n",
        "print(dataframe.select(first(\"Country\"), last(\"Country\"), first(\"InvoiceDate\"), last(\"InvoiceDate\"),min(\"InvoiceDate\"),max(\"InvoiceDate\"), countDistinct(\"InvoiceNo\")).show())\n",
        "\n",
        "print(spark.sql(\"select first( Country), last(Country),first(InvoiceDate), last(InvoiceDate) , max(InvoiceDate), min(InvoiceDate), count(distinct InvoiceNo)  from retailsales\").show())\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WcKW1VGMnyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sum of quantity, sum of revenue by Country, CUstomer, date, hour etc\n",
        "from pyspark.sql.functions import  sum\n",
        "dataframe.groupBy(\"Country\").agg(    sum(\"Quantity\").alias(\"Total Revenue\")).show()\n",
        "spark.sql(\"select Country, sum(Quantity) from retailsales group by Country\").show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13e_hRWJ070w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change datatype to date YYYY mm dd hh mm\n",
        "\n",
        "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
        "dataframe=dataframe.withColumn(\"InvoiceDate\",from_unixtime(unix_timestamp('InvoiceDate', 'MM/dd/yyy hh:mm')))"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiF0Zv38wk0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract hour\n",
        "from pyspark.sql.functions import hour\n",
        "dataframe=dataframe.select(\"*\",hour(\"InvoiceDate\").alias(\"Hour\"))\n"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spn3yChX4vsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import  sum\n",
        "dataframe.groupBy(\"Hour\").agg(    sum(\"Quantity\").alias(\"Total Revenue\")).show()\n",
        "#spark.sql(\"select Country, sum(Quantity) from retailsales group by Country\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLSeeALv7bN8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "7dfad135-54d6-4f1b-fb68-e8e20fa25b31"
      },
      "source": [
        "from pyspark.sql.functions import month, quarter\n",
        "from pyspark.sql.functions import sum\n",
        "#dataframe.groupBy(\"InvoiceDate\").agg(sum(\"Quantity\")).show()\n",
        "dataframe.groupBy(month(\"InvoiceDate\")).agg(sum(\"Quantity\")).show()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+-------------+\n",
            "|month(InvoiceDate)|sum(Quantity)|\n",
            "+------------------+-------------+\n",
            "|                12|       271213|\n",
            "|              null|      2427623|\n",
            "|                 1|       179512|\n",
            "|                 6|       155500|\n",
            "|                 3|       199155|\n",
            "|                 5|       210883|\n",
            "|                 9|       294181|\n",
            "|                 4|       171310|\n",
            "|                 8|       207967|\n",
            "|                 7|       181257|\n",
            "|                10|       338270|\n",
            "|                11|       356742|\n",
            "|                 2|       182837|\n",
            "+------------------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}